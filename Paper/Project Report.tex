\documentclass{article}
\usepackage[preprint]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amssymb}


\title{Analysis on Reinforcement Learning Methods on Navigation in a Dynamic Environment}


\author{
Callum Hendry: 100970932\\
\texttt{callumhendry@cmail.carleton.ca}
\And
Jason Dunn: 101140828\\
\texttt{jason.dunn@carleton.ca}
\And
Ujan Sen: 101171605\\
\texttt{ujansen@cmail.carleton.ca}
\And
Uvernes Somarriba: 101146733\\
\texttt{uvernes.somarribacast@cmail.carleton.ca}
}


\begin{document}


\maketitle

\section{Introduction}


\section{Methods}
\subsection{Proximal Policy Optimization (PPO)}
\label{ppo}

Proximal Policy Optimization (PPO) was introduced in 2017 [1] and has achieved widespread success in the realm of reinforcement learning. It was a successor to Trust Region Policy Optimization (TRPO) introduced in 2015 [2] which used stochastic gradient ascent by using a trust region constraint to regulate between the old policy and the new updated policy. PPO is considered state-of-the-art and is the default reinforcement learning algorithm used at OpenAI because of its ease of use and significantly better and faster performance than its counterparts. 

The biggest difference between PPO and TRPO is that PPO uses a clipping function which effectively ensures that the new learned policy does not deviate more than a certain amount from the old policy. This is done by first calculating two different sets of surrogates for the policy network. The objective for PPO is calculated as follows [1]:

\begin{center}
$L_{\theta} = \mathbb{E}_{t}[min(r_{t}({\theta})A_{t}, CLIP(r_{t}({\theta}), 1-\epsilon, 1+\epsilon)A_{t})]$
\end{center}

For the surrogates, the ratios are first calculated between the predicted actions at the current state given the old policy and the new predicted actions. The first surrogate is calculated by weighting the ratios by the calculated advantages. The second surrogate is calculated in a similar way except the ratios are first clipped between 1 - $\epsilon$, and 1 + $\epsilon$ where epsilon is defined as the clip ratio, and then weighted with the advantages. The loss is then defined to be the minimum of these surrogates.

Advantages are calculated using Generalized Advantage Estimation (GAE) introduced in 2015 [3]. GAE is an improvement upon traditional advantage estimation methods because of the introduction of the $\lambda$ term which allows a trade-off between bias and variance. A $\lambda$ of 0 reduces GAE to a one-step estimation which is just standard advantage estimation whereas a $\lambda$ of 1 considers rewards infinitely into the future. It takes into consideration the temporal difference of not only the immediate rewards but also the expected future rewards. The formula for GAE calculation is as follows [3]:

\begin{center}
	$\hat{A}^{GAE}_{t} = \sum_{k=0}^{\infty}(\gamma.\lambda^{k}).\delta_{t+k}$
\end{center}
\begin{itemize}
	\item $\hat{A}^{GAE}_{t}$ is the GAE at time step $t$
	\item $\gamma$ is the discount factor
	\item $\lambda$ is the GAE parameter, the tradeoff between bias and variance
	\item $k$ is the time step offset
	\item $\delta_{t+k}$ is the advantage at time step $t+k$ calculated using the one step standard advantage calculation
\end{itemize}

The intuition behind choosing PPO is the same as TRPO: "being safe". Since the loss function is defined as the minimum of the two surrogates, the objective becomes a lower bound of what the agent knows is possible. It approaches the task being a pessimist, which has often times proved to be more beneficial than being optimistic with little chances of recovery. TRPO's objective function achieves a similar thing but is quite different in computation [2].

\begin{center}
	$L_{\theta} = \mathbb{E}_{t}[D_{KL}(\pi_{\theta_{old}}(.|s_{t}) || \pi_{\theta}(.|s_{t}))] \leq \delta$
\end{center}

The benefit PPO gives over TRPO is the usage of clipped ratios and the surrogates. TRPO enforces a strict trust region constraint where the KL-divergence between old policies and new policies is small enough, within a parameter $\delta$ leading to a second-order optimization problem [2][4]. PPO effectively does the same thing using the clipped ratio and taking the minimum of the surrogate losses resulting in a first-order optimization problem. This leads to PPO utilizing fewer computation resources while providing better results.

The PPO implementation was performed using the library stable\textunderscore baselines3 available  \href{https://stable-baselines3.readthedocs.io/en/master/}{here}. It ran with default arguments as described in the documentation.

\subsubsection{Environmental Considerations}
\label{considerations}
The environment was modified and the results for each modification of the environment is mentioned in the Results section.
\begin{itemize}
	\item The targets were reduced from multiple to single. 
	\item Running traffic lights, making u-turns, and going out of bounds were always penalized instead of it being probabilistic
	\item All penalized actions penalized to the same amount
\end{itemize}

\section{Results}
\label{results}
In this section we will discuss the results for the various implemented algorithms:

\begin{table*}[h]
	\centering
	\caption[]{Results for PPO}\label{Results for PPO}%
	\begin{tabular}{cccccc}
		\toprule
		Target & Reward Type & Reward Scaling\footnotemark[1] & Agent Score\footnotemark[2] & Random Score\footnotemark[2] & Train Time\footnotemark[3]\\
		\midrule
		Single &  Deterministic & Same & 875 & -396 & 15\\
		Single & Deterministic & Different\footnotemark[4] & 935 & -369 & 15\\
		Single & Probabilistic & Different & 890 & -334 & 20\\
		\midrule
		Multiple & Deterministic & Same & 205 & -405 & 20\\
		Multiple & Deterministic & Different & 379 & -390 & 20\\
		Multiple & Probabilistic & Different & 255 & -410 & 30\\
		\bottomrule
	\end{tabular}
	\label{tab: PPO_Table}
\end{table*}
\footnotetext[1]{Only negative rewards are altered, positive rewards remain the same}
\footnotetext[2]{Scores are out of 1000 for single target}
\footnotetext[3]{In minutes}
\footnotetext[4]{Stop at green: -2; Stop at node that is not traffic light: -2; Everything else: -5}


\section{Discussion}
\label{discussion}
From the results, it is clear that for PPO, the more complex the environment becomes, the worse the agent performs. This is expected as it is quite difficult to find an optimal solution to our problem considering how the environment behaves. For the environment dynamic when there is a single target, there does not seem to be large of a difference in performance. The higher score for different reward scaling compared to same can be attributed to the fact that when it makes mistakes, the penalty is lower in the different reward scaling scenario, which is coincidentally also the case for the multiple targets scenario. It is worth noting that the total score for multiple targets is difficult to say since the number of targets is not fixed but it is more than 1000 since there is always at least one target.

An interesting idea would be to fix the traffic lights instead of randomly initializing them. This seems quite intuitive and in fact, could even be considered closer to reality since if the environment is a representation of an urban city setting and we consider that it is the same city, traffic lights and their locations will not suddenly change. And even if they do change, they should remain consistent for a significant time once they do. Perhaps this is an avenue worth exploring where traffic lights are initially fixed but can randomly change after a certain amount of time steps. 

In the multiple target scenario, the agent performs significantly worse than the best agent when penalties are probabilistic which makes sense since this is a difficult environment dynamic to learn. Whenever probabilistic elements are introduced into the environment dynamics, it makes training significantly tougher since even though it might revisit a state and take the same action as it did last time, it might receive a different reward.

Furthermore, it is worth noting that multiple targets yield worse results than singular targets. This is expected because of the fact that when there are multiple targets, there is no way to encode direction, which was something we were able to do when there was a single target. For example, if there was a single target, we had a 1D array indicating if the target was to the up, down, left, right of the agent. But when there are multiple scattered targets, this sort of information is difficult to encode. However, given more time and resources, to achieve an optimal policy, we would like to perhaps train another agent that has the task of selecting the next target our agent should try to reach, effectively reducing the target space to a singular target again. This could be achieved via multiple methods with the most na\"ive method being just picking the target closest (in terms of Manhattan distance) to where the agent is. A further, more refined method would be allocating each target with a priority and then choosing the next target based on the priority. A further refinement would include a combination of the two.

Given more time and resources, we would also like to test out more scenarios including various combinations of the scenarios mentioned in \ref{considerations}. We would also like to test various reinforcement learning algorithms and their performance if the targets had priorities assigned to them. We would also like to add pedestrians and random events in the environment and evaluate performance.

\section*{References}


[1] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., \& Klimov, O. (2017). Proximal Policy Optimization Algorithms. arXiv preprint \href{https://arxiv.org/abs/1707.06347}{arXiv:1707.06347}.

[2] Schulman, J., Levine, S., Moritz, P., Jordan, M. I., \& Abbeel, P. (2017). Trust Region Policy Optimization. arXiv preprint \href{https://arxiv.org/abs/1502.05477}{arXiv:1502.05477}.

[3] Schulman, J., Moritz, P., Levine, S., Jordan, M., \& Abbeel, P. (2018). High-Dimensional Continuous Control Using Generalized Advantage Estimation. arXiv preprint \href{https://arxiv.org/abs/1506.02438}{arXiv:1506.02438}

[4] Wang, Y., He, H., Wen, C., \& Tan, X. (2020). Truly Proximal Policy Optimization. arXiv preprint \href{https://arxiv.org/abs/1903.07940}{arXiv:1903.07940}.

\end{document}